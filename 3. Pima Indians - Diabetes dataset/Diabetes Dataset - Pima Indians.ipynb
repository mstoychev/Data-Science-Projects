{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e80521-6b5b-40c2-9655-22f061bc0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, make_scorer, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11717a12-9737-4a44-a174-0666ff664356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867da8e-b0d7-4f4d-b090-1cd4c8e44140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress LightGBM specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm.basic')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm.engine')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm.callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e7e7a-ae8c-4fb9-9400-f7a9b0619da9",
   "metadata": {},
   "source": [
    "# Pima Indians - Diabetes Dataset\n",
    "https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data\n",
    "\n",
    "**Mitko Stoychev**\n",
    "11/05/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb32331-da31-47bc-9027-38633b6914f9",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8bf73-c569-465c-b466-fd0118613301",
   "metadata": {},
   "source": [
    "**Context about this notebook**\n",
    "This is one of my first data science projects. I will try to make an exhaustive EDA and then apply different machine learning models. I am open to any criticism and suggestions. I am just starting on this data journey, so I assume that this analysis could have some errors or many possible improvements.\n",
    "\n",
    "**Context about the dataset**\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "**What is Diabetes ?**\n",
    "According to FDA, Diabetes is a chronic (long-lasting) health condition that happens when the level of glucose in blood (blood sugar) is too high. Insulin is a hormone made by the pancreas that helps move sugar out of blood and into cells for energy. An estimated 30 million people in the United States are living with diabetes. About 1 in 4 people living with diabetes don’t know they have it. Diabetes disproportionately affects racial and ethnic minorities and other diverse groups.\n",
    "\n",
    "In the article \"Diabetes: Living with a Chronic Disease\" we can quote the following:\n",
    "\n",
    "- \"The year 2021 marked the 100th anniversary of the discovery of insulin. Before insulin was discovered, people with diabetes didn’t live long lives. Since then, we’ve come a long way in reducing the toll diabetes takes on people’s daily lives. But the fight isn’t over.\n",
    "\n",
    "- Today, the number of people with diabetes is higher than it has ever been. And it’s not just your grandparents you have to worry about. People are developing diabetes at younger ages and at higher rates. But the more you know about diabetes, the more you can do about preventing it, delaying it, or lessening its harmful effects.\"\n",
    "\n",
    "![Texto alternativo](https://cdn1.byjus.com/wp-content/uploads/2018/11/biology/2017/08/27111256/0diabeties001.jpg)\n",
    "\n",
    "\n",
    "In the following notebook, I will try to analize which factors have more importance to diabetes disease. \n",
    "\n",
    "In the dataset, we can find the next features:\n",
    "* 1. Pregnancies: number of times pregnant\n",
    "* 2. Glucose: express the Glucose level in blood, Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "* 3. BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "* 4. SkinThickness: Triceps skin fold thickness (mm)\n",
    "* 5. Insulin: express the Insulin level in blood, 2-Hour serum insulin (mu U/ml)\n",
    "* 6. BMI: Body Mass Index, (weight in kg/(height in m)^2)\n",
    "* 7. DiabetesPedigreeFunction: Diabetes pedigree function to express the Diabetes percentage, \n",
    "* 8. Age: age (years)\n",
    "* 9. Outcome: express the final result 1 is YES o is NO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115b043-2c68-4904-8fe7-72bb0d6bdb3a",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8108aa5-0a71-4d80-a41f-5d61b25da470",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = pd.read_csv(\"data/diabetes.csv\")\n",
    "data_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ee676-47b7-4467-8f80-2e6ee4e71b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Practice → change column names to snakecase and lowercase\n",
    "data_original.rename(columns={\n",
    "    'Pregnancies': 'pregnancies',\n",
    "    'Glucose': 'glucose',\n",
    "    'BloodPressure': 'blood_pressure',\n",
    "    'SkinThickness': 'skin_thickness',\n",
    "    'Insulin': 'insulin',\n",
    "    'BMI': 'bmi',\n",
    "    'DiabetesPedigreeFunction': 'diabetes_pedigree_function',\n",
    "    'Age': 'age',\n",
    "    'Outcome': 'outcome'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bba57-c314-4d36-84e7-b721e08f62dd",
   "metadata": {},
   "source": [
    "## 3. First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc771e-0f7d-49b5-91e2-3125858709bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0abfa-4c7c-44bd-b1d8-91fb195fba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffd68f-db63-465c-903a-905c4da11871",
   "metadata": {},
   "source": [
    "At first glance I can see that the dataset is very small (768 entries) and there are **not** any **null values** in the dataset. That´s good because I don´t have to clean them.\n",
    "\n",
    "However, we can see that there are **min values** equal to **zero**. This means that they are non-null, but empty data. This could confuse the model. I would have to deal with it later. \n",
    "\n",
    "Another point that catches my attention is the Insulin. The mean is 78.7 and the standard deviation is 115.2. This is very strange at first glance. It may be my ignorance on the subject. I would have to do more research on the behavior of insulin in the human body.\n",
    "\n",
    "Another point to note is that some medical values have a minimum value equal to zero. It is totally impossible with the life of the human being. I will have to deal with it later.\n",
    "\n",
    "Anyway let us explore the \"outcome\" column too take a first look for the diabetic distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b627c-fea6-409f-b9c0-75781513e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTCOME COLUMN  → Check diabetes distribution among the patients \n",
    "data = data_original.copy()\n",
    "\n",
    "data[\"outcome_cat\"] = data[\"outcome\"].map({0: \"non-diabetic\", 1: \"diabetic\"})\n",
    "outcome_counts = data[\"outcome_cat\"].value_counts()\n",
    "labels = outcome_counts.index\n",
    "counts = outcome_counts.values\n",
    "\n",
    "data.drop([\"outcome_cat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337547eb-331c-4dec-9664-8345b563fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Diabetes Distribution')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5767bd-03bb-499c-b56c-65cee09782b9",
   "metadata": {},
   "source": [
    "This dataset has approx. 35% of the population with a diabetic disease.\n",
    "\n",
    "This is striking to me because the proportion of diabetes in the world is of 10.5% of the adult population (20-79 years) according to the IDF Diabetes Atlas (2021) reports. In any case, I will keep it in mind this initial exploration highlights a potential class imbalance within the data.\n",
    "\n",
    "Before proceeding to the exploration of the data, I am concerned about data equal to zero. It is logical that these rows are not real medical values, but they are empty and may confuse my analysis. I will try to manage them in such a way that they do not hinder my exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95670316-7056-4d5c-80d6-aa5075699d6d",
   "metadata": {},
   "source": [
    "## 4. Outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad991b7-df2f-4843-909d-82868ee39ed1",
   "metadata": {},
   "source": [
    "At first, I will use Tukey’s method, which is widely used for exploratory data analysis and has become a standard approach for identifying outliers in statistical and data analysis practices. John Tukey looks at the data’s spread using the Interquartile Range (IQR).\n",
    "\n",
    "After that I will take a look at the features: \"glucose\", \"blood_pressure\", \"skin_thickness\", \"insulin\" and \"bmi\" equal to zero. It´s impossible value with the existence of human beings. How many records do we have with this condition?\n",
    "\n",
    "Since the dataset has clear errors when recording the data, first I thought of checking if we have duplicate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe0bf0-f59b-442c-91b5-52d9de7b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d4899-1afb-40bf-a500-e8171d9b9b79",
   "metadata": {},
   "source": [
    "We can see that we have no duplicate rows.\n",
    "\n",
    "So let´s use the IQR to drop the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272248c7-2298-467e-bce7-1bd3b8c9e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(data, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a list of the columns to apply Tukey's method for detecting outliers.\n",
    "    Returns a dictionary where keys are column names and values are lists of indices considered outliers.\n",
    "    \"\"\"\n",
    "    outliers = {}\n",
    "    for feature in features:\n",
    "        q1 = data[feature].quantile(0.25)\n",
    "        q3 = data[feature].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        feature_outliers = data[(data[feature] < lower_bound) | (data[feature] > upper_bound)].index.tolist()\n",
    "        outliers[feature] = feature_outliers\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107d2b5-ef6d-4315-9992-03bba6bdc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_to_drop = outlier_detection(data, [\"glucose\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \"bmi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbd7f0-9050-484b-8f45-0fe03d7bdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_indices = set()\n",
    "total_outliers = 0\n",
    "\n",
    "# Collecting unique indices and counting outliers\n",
    "for indices in outliers_to_drop.values():\n",
    "    unique_indices.update(indices)\n",
    "    total_outliers += len(indices)\n",
    "\n",
    "unique_indices_list = list(unique_indices)\n",
    "unique_count = len(unique_indices_list)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03444d-2404-4b39-8f79-2fa2cb8bb0f5",
   "metadata": {},
   "source": [
    "Having 93 outliers out of a dataset with 768 instances is indeed a substantial number, constituting around 12% of the data.\n",
    "For obvious reasons I cannot delete all the rows.\n",
    "\n",
    "Given these considerations, it's advisable to analyze each feature individually and decide on the appropriate action for handling outliers. I will document decision-making process to justify any data transformations in my analysis.\n",
    "\n",
    "So, let's see how we manage this problem. Let´s see if correlations give us an idea about the importance of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37295a-5b3f-4050-8141-7f4df4f48c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to check any correlations\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(data.corr(), annot=True, linewidth=1, cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a48aa-792c-4627-94a8-29ebf6c2bcaa",
   "metadata": {},
   "source": [
    "With the **Pearon Correlation method** is seems that only \"glucose\", \"bmi\", \"age\" and maybe \"pregnancies\" feature seem to have some low correlation with the survival probability.\n",
    "\n",
    "It **doesn't mean that the other features are not usefull**. In one hand, it is well known to all that insulin treatment is prescribed in diabetics. In the other hand, subpopulations in these features can be correlated with diabetic disease. To determine this, we need to explore in detail these features by grouping. But first let´s mange the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26709af9-4518-4d84-a64d-beaa71912cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(data.corr()[\"outcome\"].sort_values(ascending=False)[1:],label=\"Correlation for Outcome\",color=\"blue\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828525d4-d37c-4f95-8c51-00d120e6ddda",
   "metadata": {},
   "source": [
    "We can see that \"glucose\" and \"bmi\" are the most correlated with the diabetes desease. So let´s explore them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48cf1c-1d1e-483b-b3e1-09657aa40635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for impossible glucose zero values \n",
    "impossible_zeros_glucose = data[data['glucose'] == 0]\n",
    "impossible_zeros_glucose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad69a10-ca9a-4db3-b883-c834e3e7bdba",
   "metadata": {},
   "source": [
    "The impossible glucose values are row 75, 182, 242, 349 and 502. \n",
    "\n",
    "I can note insuline value is missing too. Considering that both values have impact in the diabetes prevalence and that this small dataset with 768 rows, deleting these rows might not significantly affect the analysis. \n",
    "\n",
    "For me fill these values with an invented value (arithmetic mean, for example) is worse decision. I am working with a medical dataset, so making up medical test results does not seem to be the best idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2eea8c-c9d0-4fde-a4a4-ce22c4b53878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace impossible zero values with NaN\n",
    "data['glucose'] = data['glucose'].replace(0, float('nan'))\n",
    "\n",
    "# Drop rows with empty 'Glucose' and 'Insulin' values\n",
    "data.dropna(subset=['glucose'], inplace=True)\n",
    "\n",
    "print(f\"I have already deleted {len(impossible_zeros_glucose['glucose'])} rows with glucose=0 value and the current shape is of the data is: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76944e0-0df6-4dd3-b839-e1517c57573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for impossible bmi zero values \n",
    "impossible_zeros_bmi = data[data['bmi'] == 0]\n",
    "impossible_zeros_bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a0d8b-c05a-4952-a4ba-a5be579a2010",
   "metadata": {},
   "source": [
    "Due to the importance of the correlation of the bmi and the outcome and the small number of these 11 records, I decide to eliminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc7a49-5cd5-4a3d-8e02-ffc27e415cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace impossible zero values with NaN\n",
    "data['bmi'] = data['bmi'].replace(0, float('nan'))\n",
    "\n",
    "# Drop rows with empty 'Glucose' and 'Insulin' values\n",
    "data.dropna(subset=['bmi'], inplace=True)\n",
    "\n",
    "print(f\"I have already deleted {len(impossible_zeros_bmi['glucose'])} rows with bmi=0 value and the current shape is of the data is: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aac0b3-3456-410e-a046-a94f914da1f9",
   "metadata": {},
   "source": [
    "Let´s see how much \"impossible zero values\" is left and how we can manage it. But before I will replace them with a np.nan value just to manage them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669d857-c52c-40d6-9aa1-ab4551c2b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"impossible zero\" value with \"np.nan\" value\n",
    "data[['insulin','blood_pressure','skin_thickness']] = data[['insulin','blood_pressure','skin_thickness']].replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74372b-ddb1-4cdd-9637-7ebf01457adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528783f-dbe7-47ca-b297-14ff1cdbfc79",
   "metadata": {},
   "source": [
    "I am very **concerned about the \"insulin\" column**. The **missing values** reprensent almost **48% of the data** and we must not forget the importance of that hormone. It is well known about as a universal way to control diabetes with its measurements at the blood level. I am very concerned about filling this value with an \"invented value\" (mean, median or mode). I find it very worrying and it can lead to errors in the analysis.\n",
    "\n",
    "To fill these Nan values the data distribution needs to be understood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43fe82-0cb0-4fac-b982-d058c5cf8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_results = data[[\"insulin\", \"skin_thickness\", \"blood_pressure\"]].apply(lambda x: x.skew())\n",
    "print(skewness_results)\n",
    "\n",
    "data.hist([\"insulin\", \"skin_thickness\", \"blood_pressure\"], figsize=(6,3), layout=(1, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddf669-2266-4265-84bc-a56085353349",
   "metadata": {},
   "source": [
    "Based on my analysis of the data and considering the characteristics of each feature, my considerations about handling missing values are the next:\n",
    "\n",
    "**1. Insulin**\n",
    "\n",
    "Since insulin levels are crucial for managing diabetes, it's **risky to fill missing values with invented values**. Given the significant number of missing values (almost 48%), deleting the column might be a viable option, especially if there are other features that can adequately capture the information insulin would provide. Alternatively, I could consult with a domain expert for guidance, but since I do not have access to this option, I choose to eliminate the \"insulin\" column.  ***→ drop insulin column***\n",
    "\n",
    "\n",
    "**2. Blood Pressure**\n",
    "\n",
    "With only 28 missing values and a low skewness, filling them with the arithmetic mean seems reasonable. I could also consider using more sophisticated imputation methods, such as K-nearest neighbors (KNN) imputation, which takes into account the relationships between features. But in this particular case it seems that it is not worth it and does not give more value to the data. ***→ fill with the arithmetic mean***\n",
    "\n",
    "\n",
    "**3. Skin Thickness**\n",
    "\n",
    "Filling missing values with the median might be a reasonable approach, given the relatively high skewness, as the median is less affected by extreme values. Considering the potential correlation  of the triceps skin fold thickness (mm) with the body mass index, I could explore filling missing values based on the relationship between \"skin_thickness\" and \"bmi\".  ***→ fill with the arithmetic mean***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c3927-4d1a-4508-9662-955a02ed7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Insulin\n",
    "data.drop([\"insulin\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e85be0-1d64-498e-8d50-1c8a4892eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Blood Pressure\n",
    "data[\"blood_pressure\"].fillna(data[\"blood_pressure\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dfb0b1-b080-4f3a-8be1-73f49a892ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Skin thickness\n",
    "# Impute missing values for skin thickness using K-nearest neighbors (KNN) imputation\n",
    "def impute_skin_thickness(data):\n",
    "    impute_data = data[[\"skin_thickness\", \"bmi\"]]\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=5) \n",
    "\n",
    "    # Perform imputation\n",
    "    imputed_values = imputer.fit_transform(impute_data)\n",
    "\n",
    "    # Replace original skin_thickness values with imputed values\n",
    "    data[\"skin_thickness\"] = imputed_values[:, 0]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8adc57-af05-451d-88bb-f8a05bad5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the method and impute missing values for skin thickness\n",
    "data = impute_skin_thickness(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00de2bc-e77a-464c-bbf5-0fe59451ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d557a-96a4-4b1f-8dcd-d0fca6919d61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2e313-d252-4115-bdf1-2fcc78dd8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to check any correlations\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(data.corr(), annot=True, linewidth=1, cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe0d18-cc33-4217-adec-fff93cad65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"pregnancies\"\n",
    "data.groupby([\"pregnancies\"])[\"outcome\"].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f7d9c-dad5-4893-bff1-abc22e0ec03e",
   "metadata": {},
   "source": [
    "Women with higher numbers of pregnancies, particularly 7 or more, tend to have a higher mean outcome of diabetes. This indicates that having a higher number of pregnancies might be associated with an increased risk of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5a2bb-80c4-4b87-9486-b384eab9730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"glucose\"\n",
    "\n",
    "# Define bins for glucose levels\n",
    "bins = [0, 100, 125, 150, 200, float('inf')]\n",
    "labels = ['<100', '100-125', '125-150', '150-200', '>=200']\n",
    "\n",
    "# Group by glucose bins and calculate mean outcome\n",
    "grouped_data = data.groupby(pd.cut(data['glucose'], bins=bins, labels=labels, right=False), observed=True)['outcome'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x='glucose', y='outcome', data=grouped_data)\n",
    "plt.title('Probability of having a positive outcome given a specific range of glucose levels')\n",
    "plt.xlabel('Glucose Level')\n",
    "plt.ylabel('Mean Outcome')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c2512-0b27-49e9-a37e-0f1361d0d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Glucose Distribution\n",
    "sns.histplot(data[\"glucose\"], color=\"m\", kde=True)\n",
    "plt.title(\"Glucose Distribution\")\n",
    "plt.xlabel(\"Glucose\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.text(150, 75, \"Skewness: %.2f\" % data[\"glucose\"].skew(), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277e74b-59c9-4596-b687-1220d61907fa",
   "metadata": {},
   "source": [
    "Despite being slightly right-skewed, the distribution appears relatively symmetrical overall, with a single peak and a gradual decrease in frequency as glucose levels increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071986c-9670-40c8-9b8e-16dec8199ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Blood Pressure Distribution\n",
    "sns.histplot(data[\"blood_pressure\"], color=\"r\", kde=True)\n",
    "plt.title(\"Blood Pressure\")\n",
    "plt.xlabel(\"Blood Pressure [mm Hg]\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.text(95, 60, \"Skewness: %.2f\" % data[\"blood_pressure\"].skew(), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3625f-a6c4-4378-93ca-4244f4aacc1a",
   "metadata": {},
   "source": [
    "The distribution of blood pressure values appears to be relatively symmetric or slightly right-skewed, as indicated by the skewness value of 0.14. This suggests that there is a roughly equal spread of blood pressure values on both sides of the mean.\n",
    "\n",
    "Note that I have filled 28 missing values with the mean and this has influenced the distribution by shifting it slightly towards the mean value. This is reflected in the skewness value, which is close to zero, indicating a more symmetric distribution compared to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a12011-5511-4c38-a94a-dbc3a3fb7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Body Mass Index Distribution\n",
    "sns.histplot(data[\"bmi\"], color=\"b\", kde=True)\n",
    "plt.title(\"Body Mass Index (BMI)\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.text(50, 50, \"Skewness: %.2f\" % data[\"bmi\"].skew(), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45231044-a3af-49b5-b3ae-dbfa4a549c3a",
   "metadata": {},
   "source": [
    "The positive skewness value 0.60 indicates that the distribution of BMI values is moderately right-skewed. This means that the tail of the distribution extends more towards higher BMI values, and there are relatively fewer instances of very low BMI values compared to very high ones.\r\n",
    "\n",
    "*Note:* **The normal BMI according to World Health Organization is 18.5-24.9.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The right-skewed distribution of BMI values suggests that there may be a larger proportion of individuals with higher BMI values in the dataset. This information is valuable for understanding the distribution and characteristics of BMI values, which are important indicators of overall health and risk factors for various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c80ca-adac-43bb-984a-0c67e403dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring skin_thickness Distribution → ORIGINAL DATA\n",
    "sns.histplot(data_original[\"skin_thickness\"], color=\"black\", kde=True)\n",
    "plt.title(\"ORIGINAL DATA → Skin Thickness\")\n",
    "plt.xlabel(\"Skin Thickness\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.text(70, 50, \"Skewness: %.2f\" % data_original[\"skin_thickness\"].skew(), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3be72-54c2-4d91-8183-29cd3d0ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring skin_thickness Distribution\n",
    "sns.histplot(data[\"skin_thickness\"], color=\"g\", kde=True)\n",
    "plt.title(\"CLEAN DATA → Skin Thickness\")\n",
    "plt.xlabel(\"Skin Thickness\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.text(70, 80, \"Skewness: %.2f\" % data[\"skin_thickness\"].skew(), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739bf2e-6133-4b78-a364-4c687bc12687",
   "metadata": {},
   "source": [
    "In the **original data histogram**, the distribution of skin thickness in the original data (before cleaning) appears to be approximately symmetric or slightly right-skewed, likely **due to the presence of zero values** representing missing or invalid measurements. This is totally wrong.\n",
    "\n",
    "The **cleaned data histogram**, the distribution of skin thickness in the cleaned data shows a **noticeable right skew**, indicating that the removal or adjustment of false zero values led to a change in the shape of the distribution. The skewness value of 0.71 for the cleaned data confirms a right skew in the distribution of skin thickness after addressing the false zero values, reflecting a more accurate representation of the data. \n",
    "\n",
    "This result gives me a lot of confidence, because BMI is highly correlated with body composition and is clearly related to \"skin_thickness\" or Triceps skin fold thickness (mm). These two medical estimates are used as a tool to measure body fat and therefore the state of health of humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a289e-a018-4bf5-8758-3a84367c1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Diabetes Pedigree function\n",
    "sns.histplot(data[\"diabetes_pedigree_function\"], color=\"y\", kde=True)\n",
    "plt.title(\"Diabetes Pedigree Function\")\n",
    "plt.xlabel(\"Diabetes pedigree function\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "print(data[\"diabetes_pedigree_function\"].skew())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb3248-d14c-4c9d-a3f8-955a30d74630",
   "metadata": {},
   "source": [
    "The Diabetes Pedigree Function (DPF) is a numerical score that quantifies the diabetes mellitus history in relatives of individuals who are part of the Pima Indian population. It provides a measure of the likelihood of diabetes based on family history. This function is used as a predictor or feature in diabetes prediction models.\n",
    "\n",
    "The right-skewed distribution of Diabetes Pedigree Function scores suggests that there may be a larger proportion of individuals with lower scores (indicating a lower likelihood of diabetes based on family history) in the dataset. However, the presence of outliers in the right tail of the distribution indicates that there are also individuals with higher scores, suggesting a higher likelihood of diabetes based on family history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31603c-f7f3-47da-b331-3baf843f9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Age \n",
    "sns.displot(data, x=\"age\", col=\"outcome\", bins=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90017bc0-297d-40b2-ae39-a82642896be5",
   "metadata": {},
   "source": [
    "The proportion of diabetes outcomes being only 35% indicates that diabetes is not highly prevalent in the dataset, but it still a really big prevalence for an desease.\n",
    "\n",
    "Both histograms show a decreasing trend in prevalence as age increases. But this **is NOT suggesting that older individuals are less likely to have diabetes compared to younger individuals**. The reality is that we have more young age participants in the dataset than older ones.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b5876-f9fb-4d21-bcce-4a751f757f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a866d-6b4a-41d4-bf0a-f38277e6c9eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02faa77-bf88-4965-8d57-5e872d0991c1",
   "metadata": {},
   "source": [
    "Scaling data is a crucial step in many machine learning algorithms because it ensures that features contribute equally to the model's performance. This process adjusts the range of feature values so that no single feature dominates the others due to its scale.\n",
    "\n",
    "**1. Equal Contribution of Features and Distance-Based Algorithms**:\n",
    "   \n",
    "Different features in your dataset may have different units or scales. For instance, in a dataset with features such as \"age\" (0-100 years) and \"income\" (0-100,000 dollars), the range of values varies significantly. Without scaling, features with larger ranges can dominate the distance calculations and model training. Algorithms that rely on distance metrics (e.g., k-nearest neighbors (KNN)) are particularly sensitive to the scale of data. If one feature has a larger scale, it can disproportionately influence the calculation of distances, leading to biased results.\n",
    "\n",
    "**2. Improving Convergence of Gradient Descent**:\n",
    "\n",
    "Algorithms like gradient descent converge faster with scaled data. When features are on similar scales, the optimization process can progress more smoothly and reach the minimum more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9711f-a2a9-4d13-973d-fc620841fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy(deep=True)"
   ]
  },
  {
   "attachments": {
    "01aa8a66-f557-436d-bd26-bcd8ce76819e.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAACdCAIAAAD5bR6iAAASfUlEQVR4Ae2de0wU1x7Hj8uruoqWKo++uGkirW0xUKqSGBupNlUgxZYC12qhlVaC2ooglUfT2lKDD4oVYqJCU4PWqkSLqVKtMfVqaSk+SoVWQE0pUhYBhWWXfbqzt9fJPXcuuyu745Cd2fmeP8jsmd858/t9fme+Gc7MnCFWFBAAARBwnQBxvQlagAAIgIAV2oFBAAIgwIcAtIMPNbQBARCAdmAMgAAI8CEA7eBDDW1AAASgHRgDIAACfAhAO/hQQxsQAAFoB8YACIAAHwLQDj7U0AYEQADagTEAAiDAhwC0gw81tAEBEIB2YAyAAAjwIQDt4EMNbUAABKAdGAMgAAJ8CEA7+FBDGxAAAWgHxgAIgAAfAtAOPtTQBgRAANqBMQACIMCHALSDDzW0AQEQgHZgDIAACPAhAO3gQw1tQAAEoB0YAyAAAnwIQDv4UEMbEAABaAfGAAiAAB8C0A4+1NAGBEAA2oExAAIgwIcAtIMPNbQBARCAdmAMgAAI8CEA7eBDDW1AAASgHRgDIAACfAhAO/hQQxsQAAFoB8YACIAAHwLQDj7U0AYEQADagTEAAiDAhwC0gw81tAEBEIB2YAyAAAjwIQDt4EMNbUAABKAdGAMgAAJ8CEA7+FATeRuj0VhfX//ee+8lJibGxcWtXr36p59+MplMrNs6ne7gwYNpaWmxsbHLli2rra21WCwij8gZ986cObP4Trl27Zoje7PZfPny5TNnzlAajixRPyIBaMeIiCRmcOPGjbS0NIVCQTjF29s7OzvbZDL19fUlJCRw9hCFQlFQUGA0GiUW5/+7q1KpgoOD2bhycnL+f+f/fpWWlnp7exNCVq5cCfn4HxdeW9AOXtjE2qinp2f27NmEkOeff/7IkSPd3d0XLlyYO3cue1J98MEHsbGxhJDFixdfvXq1o6Nj+fLlhPxHPs6cOSPWmJzyq7q6mgpiWVmZ3TYajeaxxx5jzV5++WVoh11KzldCO5xnJXZLnU4XFxdHCFm7dq1Wq6XuNjc3sycMezGSlJSk0+nMZnNGRgY93z788ENqL8WNlStX0ljOnz9vN4RffvmF2mzYsMGuDSqdJwDtcJ6VqC0ZhikrKyOEpKWl6fV6rq9qtdrPz489bcaOHdva2mq1Wuvq6saMGeMZ59LQ0FBkZCQbS0hIyODgIDd8ur1582Yar9Svs2hQbtyAdrgRvpCHvnHjRnBw8OTJk//6669h/ba1tdFzJjY2lp3aOHToEK309vZuaGgY1kpCP9vb2728vNhwFi1aZDabbZ03mUyLFi1ibQICAgYGBmxtUOMSAWiHS7jEa7xr1y5CSGZmpu1Nk/3791OZ+OSTT9gYurq6nn76aULI2LFji4qKbt++Ld7YRvJs9+7dNMDi4mK75mq1OigoiDV78cUXDQaDXTNUOk8A2uE8K1Fb7t+/Pykpye6l+LvvvktPrZ9//pmG0d/f39DQ0NXVxUM49Hp9j3Clu7ub98ylxWJJT0+3GyCN1Gq1/vDDD9SmsLCQuwvb/AhAO/hxE2Mr2ysOq9Wq0+mio6PZ0+bhhx92NBfgajxZWVn0VBRkIzs721UfWHutVsteQBFC7jLZUVRURP2sra3ldyy04hKAdnBpeOB2V1eXr68ve9okJyfbnQvgEXZRUdHMmTNnCVcqKyt5uGG1Wq9evUonfR3deTUajewdKEKIUqns6+vjdyy04hKAdnBpeOA2d7Jj69atnhdhRUUFvaDYtGmT3QDVavXkyZNZszlz5gy7D2W3CSpHJADtGBGRhA0YhsnMzKSnVmNjo4SDsee6xWJ58803aYDc2Ryu+enTp6lNVlYWwzDcvdjmRwDawY+bNFpxH3ywO9nBMIxGo5HuTQetVvvkk0+yuhAYGOhoNuejjz6i2lFdXc0mz2QycZ+gk0ZGxeQltENM2bgHX8xmc2NjY0VFxYkTJ+h9E+6DDwkJCbb3Mi5dujRz5sysrCyJXsZfuXKFTnbMnz/fNkCr1Wo0GhcuXMhqh4+PT2dnp9VqZRimsLAwMjLy3Llz90Bd1k2hHZ6QfoZhSkpK2DnRMWPGHDx4kI2KOxdg96nz4uLiv0+q9evXS5QCN8DXX3/d7j8jAwMDAQEBrHZERUUNDQ1ZrVatVhseHq5QKNrb2yUau9vdhna4PQUCONDR0TF+/Hh6WZ6bm2u1Ws1mc1JSEq08fPjwsCP19/dPnTpVqVS2tbUN2yWJnxaL5Y033qABpqWl2XW7vLyc2mRkZLB3spuamgghs2bN0ul0dluhckQC0I4REUnA4JtvvqGnByFkz549Vqu1vb19woQJtL6+vp4byd8Pd61YsYIQkpOTQ//H4RqIf1ur1U6bNo0GmJCQYHsH+uTJk1xV3b59O/2HhRCyefNm8YcpWg+hHaJNjQuOcR+aXLp0qclk0ul0KSkphJBJkyaxZxd72rCdqtVq9sXTmTNn3rp1y4UjicmUO9lBCHnggQdaWlqogxaL5ezZs6GhoX5+fnQ1E/YudU9Pz6OPPhocHKxSqag9NlwlAO1wlZgY7TUazfTp0wkhM2bMOHr06MmTJ9n3vubOnXvixAmlUkkICQ4OLi8v//HHHysrK9knTSMiIv78808xxuOcT9zJDlYfn3322X379jU0NNTW1mZmZiqVyilTpuzdu5e+BTd79uxvv/02Pj7+74fEtmzZYnd+xLmDw8oK7fCQQXD69Gm6cBZ7IiUmJvb29jIMU1NT88gjj9Bre0KIn5/f8uXLe3p6pBu8xWJZtmwZDWrevHnsgmC0xsvL65VXXmlpaWEYpru7e/78+XQXISQ1NVXqS6W5PXfQDrenQDAHOjs7KyoqCgsLi4qKTp06xZ0FvHnzZlVVVX5+/tq1a8vLy9va2uzezhTMldHvaGho6KmnnmLl4MEHH1Sr1deuXauoqMjNzc3KyiotLW1ubuaqg16vP378+Mcff5yXl/f1119L9J706HN14QjQDhdgwVQ8BK5du0ZnMRy9xiIebz3SE2iHR6bV84P64osv6P8gjl5j8XwKbo0Q2uFW/Dg4LwIWi+Xtt9+m2uHoNRZefaORswSgHc6Sgp14COh0uoiICFY7QkJC1Gq1eHyTjyfQDvnk2nMi7ejooAuUJiYm2j4S5jmhijgSaIeIkwPXHBDgLkpSWlrqwArVo0sA2jG6fNH7aBDgrnh46dKl0TgE+hyRALRjREQwEB2BJUuWsJMdWATMjbmBdrgRPg7Nk8Cvv/6anJycmZl55coVnl2g2T0T8Cjt0Gq1R48ezc/PX79+/cWLF4e9Hmo0Gt9///28vDw8U3jPwwYdgIAHvc/S0NAwY8YMes9/3LhxZWVl3Bn4L7/8khASHx8P7cDAB4F7J+Ah1x3nzp2bMmUKFQ66kZOTw65JWVdXFxQUFBgY+Ntvv907NfQAAiDgCdrR39/PvoFOJYO7ER4enpqaGhgYqFAo6GJ8Tib+9u3bDQ0N/xK6nD17lvualpPOwAwEREXAE7Rj586dhJApU6asW7fu+PHjdXV1X331VW5ubkxMjL+/P9WRnJwcV18ePXToEF1Kl/YjyMbGjRtFNQ7gDAi4SkDy2mEwGBYuXOjn52f7oUCDwdDV1fXCCy8QQlJSUrjvpDuJqb29PTU1NdnpkpKS8s87ZfHixa/9tyxZsmTp0qWv3ympd0p6enpzc7OTPsAMBMRJQPLaYbVar1+/3tnZafs1VoPBkJeXRwhZsGAB71ceTCaT2bly+67FwincGVxxDgt4BQIjEvAE7bAbpE6ny87OJoTExcV52Cd8ent7v0UZHQIXL160O5xQaUvAM7VjaGgoPT2dEJKYmMj7isMWlkhqamtrBZlzQSe2BF566SWRZFn8bnigdvT39ycmJhJC0tLS2A/5iD8NLnkI7bA954WqgXY4PxQ9TTtUKhW7qm12djaPyVHnwbnR8vr16xUoo0Pgu+++c2NmpXVoj9KOP/74Izo6WqFQbNiwQagHKGynYO89waPR5717hR5AwCUCnqMdra2t4eHhvr6+O3bssPscB3ujwyU6N2/eLCgoeFfosnr1anwG1aVEwFiEBDxEO5qamsLCwpRK5b59+4a9AsdC12g0GRkZa9euNRgMzqeBu8aMUP9Rs/1I9/PRztODpWcT8ATtaG5uDg0N9ff3r6qqsiscVquVfREuNjbWJe0wGAwHDhzYJ3Q5cOCAR07ievapguiGEZC8drS2toaFhfn7+x84cECj0dhqB8MwZ8+eZT+MVllZOSx+/AQBEOBHQNra0d3dHRkZSQjx8vIKCAgIDw9/7bXXtm3bdu7cuYE7pb6+fs2aNezn4KdPn67RaPhhQisQAIFhBCSsHXq9/tVXXyWE+Pj4REdHD3uV1sfHh/uB0vHjx3///ffDgsdPEAAB3gSkqh0Wi6WkpIQQEhYWVldXp9frdTrd9u3bx40bZzupOXHixJqaGt6M0BAEQMCWgFS1o7m5ecKECffff39jYyONimGYY8eODVsEKDo6uqGhAY9UUErYAAFBCEhVOw4fPhwTE7N79+5hFBiGaWlpWbVqVVxcXHp6enV1tYe9CDcsXvwEAXcRkKp2uIsXjgsCIMASgHZgJIAACPAhAO3gQw1tQAAEoB0YAyAAAnwIQDv4UEMbEAABaAfGAAiAAB8C0A4+1NAGBEAA2oExAAIgwIcAtIMPNbQBARCAdmAMgAAI8CEA7eBDDW1GJGA0Gru6umpqaj69U0pKSrZs2bLJXtm4cWNxcTEWQxoRqdgMoB1iy4jk/TEYDLW1tfHx8ffdd5/tO82OagoKCiQfucwCgHbILOGjHO7169eTk5MdCcRd6j/99NNRdg3dC0wA2iEwUDl319jY+Pjjj3MFwt/f/6GHHvLz8xtW+Y87JSQkJCgoaNq0aatWrRocHJQzOinGDu2QYtbE6PPvv/8eGhrKaoRSqVyzZs2FCxcGBgY0Gk1vb+/WrVsnTpzI7n3iiSdUKpVGo1Gr1YODg1qtFquriDGjI/kE7RiJEPY7QaCvr++ZZ55hpSEkJKS2ttZWDjZt2kSvPk6dOuVErzARNQFoh6jTIwnnTCbTihUrWF3w9fU9cuSIXbdVKpWXlxdrVlRUZNcGlRIiAO2QULJE6mpdXR1dVvqtt94ym812HVWr1eyC9YSQzMxMuzaolBABaIeEkiVGV00mU2JiIns14ePj09LS4shLlUpF/2fJyMhgGMaRJeolQQDaIYk0idfJ9vZ2+hzHggUL7H4JmPW+rq6Oase6devEGxI8c44AtMM5TrByQODzzz+nirB582YHVv+pLisro5b4QN9dQEllF7RDKpkSqZ/vvPMOVYTz58878tJsNrMf4iKEKBSKy5cvO7JEvVQIQDukkimR+kmfIvX39x8YGHDkZW9v76RJk1iVmTNnjk6nc2SJeqkQgHZIJVMi9ZNOlEZFRen1ekde7tq1i16e7Nmzx5EZ6iVEANohoWSJ0dWMjAxWFObNm2c0Gu26ODQ0FBUVxZrFxMTgosMuJclVQjsklzJxOVxZWcmKwqxZswwGg61zFovls88+Y20CAwObm5ttbVAjRQLQDilmTUQ+q1SqwMBAQsiECRM6OzuHecYwTE1NjVKpZA2OHTs2zAA/pUsA2iHd3InCc4vFUlpayl5WpKSk3Lp1i7ql0+m2bdvGCkdQUNDx48fxPBiF4wEb0A4PSKKbQzAYDMuWLWPlY/r06evXr9+xY0d+fn5ERARbGRMT09TUBOFwc56EPjy0Q2iisuzPZDLt3Llz6tSprFiwf729vZ977rmqqiqsJ+iRgwLa4ZFpdUNQDMMMDg7W19fv3bu3oqLiyJEj7e3td7lr6wYXcUhBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCUA7BMWJzkBANgSgHbJJNQIFAUEJQDsExYnOQEA2BKAdskk1AgUBQQlAOwTFic5AQDYEoB2ySTUCBQFBCfwbTF4RTRibDuAAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "98994f37-cfcb-4543-98f3-bfa0411ba419",
   "metadata": {},
   "source": [
    "![image.png](attachment:01aa8a66-f557-436d-bd26-bcd8ce76819e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa09569-e957-489a-9a84-0d2152ec1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to scale (excluding \"outcome\")\n",
    "columns_to_scale = [\"pregnancies\", \"glucose\", \"blood_pressure\", \"skin_thickness\", \"bmi\", \"diabetes_pedigree_function\", \"age\"]\n",
    "\n",
    "# Compute mean and standard deviation for each column\n",
    "mean_values = data[columns_to_scale].mean()\n",
    "std_values = data[columns_to_scale].std()\n",
    "\n",
    "# Scale the data using z-scores\n",
    "data_scaled = (data[columns_to_scale] - mean_values) / std_values\n",
    "\n",
    "# Concatenate the scaled DataFrame with the original DataFrame (excluding \"outcome\")\n",
    "data = pd.concat([data.drop(columns=columns_to_scale), data_scaled], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bd122-4872-4bc0-95a0-0d3b24f52b9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff90282-0642-466a-8fbf-2fa4fad43349",
   "metadata": {},
   "source": [
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a **test set**:  *X_test, y_test*.\n",
    "\n",
    "**Train Test Split**: Split the data into a training and test sets. We hide the test set from the algorith to have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n",
    "\n",
    "![Train Test Split](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_train-test-split_0.jpg)\n",
    "\n",
    "I assume that distribution of the target variable in the original dataset is representative of the real-world distribution for Pyma Indians, and I want to ensure that the train and test sets have a similar distribution, then using stratified sampling would be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea8044-542a-432c-bbf4-5317cb5a91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['outcome'])  # Features\n",
    "y = data['outcome']  # Target variable\n",
    "\n",
    "# Split the data into train and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30066ed-f63b-4904-a747-88469d85d8b2",
   "metadata": {},
   "source": [
    "## 8. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bb385-8098-4c91-9658-58ca7e3276e1",
   "metadata": {},
   "source": [
    "In this notebook I am going to use **10 of the most popular classifiers**:\n",
    "- Logistic Regression (LR)\n",
    "- K-Neighbors Classifier (KNN) \n",
    "- Support Vector Classifier (SVC)\n",
    "- Decision Tree Classifier (DTC)\n",
    "- Random Forest Classifier (RFC)\n",
    "- Ada Boost Classifier (Adaboost)\n",
    "- Gradient Boosting Classifier (‎GBM)\n",
    "- XGB Classifier (XGBoost)\n",
    "- LightGBM Classifier (LightGBM)\n",
    "- Cat Boost Classifier (CatBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89616118-eed7-4449-a88e-9da9835f411f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.1. Base Model Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87907bac-937d-413e-8eea-b801280608c3",
   "metadata": {},
   "source": [
    "**Cross Validation** is a widely used technique for evaluating model performance and subsequent trend checking in machine learning. It consists of partitioning the data into subsets (parts), where one subset is used for training and another subset is used to test and evaluate model performance.\n",
    "- The partitioned data is the training data (80% of the original data)\n",
    "- K-fold: the training set is randomly divided into K subsets of approximately the same size, then K iterations are performed, where K-1 subsets are for training and 1 for validation.\n",
    "- The performance computed by k-fold cross-validation is the average of all the values computed in each iteration.\n",
    "\n",
    "\n",
    "**Grid Search Cross Validation**\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"Grid Search Cross Validation\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb58a8-a965-4628-86e7-413f29e3615e",
   "metadata": {},
   "source": [
    "**scoring=\"roc_auc\"** → (**Receiver Operating Characteristic** - Area Under Curve):\n",
    "1. Definition: The ROC AUC score is a measure of a classifier’s ability to distinguish between classes. It is the area under the ROC curve, which plots the true positive rate (recall) against the false positive rate (1-specificity) at various threshold settings.\n",
    "2. Interpretation: The ROC AUC score ranges from 0 to 1. A score of 0.5 suggests no discriminative power (equivalent to random guessing), while a score of 1 indicates perfect discrimination.\n",
    "3. Usage: The ROC AUC score is particularly useful for binary classification problems and **is less sensitive to class imbalance than accuracy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8e6b8-6afe-41d4-9576-f3edc1fed4aa",
   "metadata": {},
   "source": [
    "**The confusion matrix**\n",
    "\n",
    "The confusion matrix is a technique used for summarizing the performance of a classification algorithm (binary outputs).\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*9r99oJ2PTRi4gYF_.jpg\" alt=\"The confusion Matrix\" width=\"500\">\n",
    "\n",
    "\n",
    "**Note**: This is medical datasets, the **worst case scenario is Type II error**, for obvious reasons. Minimizing Type II errors (false negatives) is crucial. A Type II error in this context means failing to identify a patient who has diabetes, which could result in the patient not receiving necessary treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0eba5-774d-4062-a8c1-f0680e3c3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977de26f-b026-4204-9d3f-d9e44d5c81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [('LR', LogisticRegression(random_state=random_state)),\n",
    "                ('KNN', KNeighborsClassifier()),\n",
    "                (\"SVC\", SVC(random_state=random_state)),\n",
    "                (\"DTC\", DecisionTreeClassifier(random_state=random_state)),\n",
    "                (\"RFC\", RandomForestClassifier(random_state=random_state)),\n",
    "                ('Adaboost', AdaBoostClassifier(random_state=random_state)),\n",
    "                ('GBM', GradientBoostingClassifier(random_state=random_state)),\n",
    "                ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)),\n",
    "                ('LightGBM', LGBMClassifier(objective='binary', verbose=-1, random_state=random_state)),\n",
    "                ('CatBoost', CatBoostClassifier(verbose=False, random_state=random_state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a859333-9292-447e-b1dc-ab5ac8e309e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_models(X_train, y_train, X_test, y_test, classifiers, scoring=\"roc_auc\"):\n",
    "    \"\"\"\n",
    "    Evaluates a set of classification models using cross-validation and plots confusion matrices.\n",
    "\n",
    "    Parameters:\n",
    "    X_train : sparse matrix, shape (n_samples, n_features)\n",
    "        The training features.\n",
    "    X_test : sparse matrix, shape (n_samples, n_features)\n",
    "        The testing features.\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "        The training target values.\n",
    "    y_test : array-like, shape (n_samples,)\n",
    "        The testing target values.\n",
    "    classifiers: list of tuples\n",
    "        List of (name, model) tuples of the classifiers to evaluate.\n",
    "    scoring : string, default: \"roc_auc\"\n",
    "        Scoring metric to evaluate the models.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function prints the mean cross-validation score for each classifier and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    for name, classifier in classifiers:\n",
    "        # Cross-validate the model\n",
    "        cv_results = cross_validate(classifier, X_train, y_train, cv=3, scoring=scoring)\n",
    "        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n",
    "\n",
    "        # Train the model on the entire training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Print confusion matrix results\n",
    "        tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "        print(f\"True positives: {tp}, False positives: {fp}, False negatives: {fn}, True negatives: {tn}\")\n",
    "        \n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "        plt.title(f'Confusion matrix for {name}', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b93b8-6337-4575-9e26-04b69aae4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models(X_train, y_train, X_test, y_test, classifiers, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e8a9a-e826-4204-bcad-22f5e88fdd7a",
   "metadata": {},
   "source": [
    "At first sight it seems that the best model is the Linear Regression roc_auc: 0.8305 (LR). However, we can see that it is the one with the most Type II errors.\n",
    "\n",
    "Therefore, taking into account the confusion matrix, I will optimize 3 models with the less number of Type II errors which are as follows:\n",
    "1. GBM → roc_auc: 0.803 (GBM) → 23 Type II errors and 11 Type I errors → *34 total errors*\n",
    "2. LightGBM → roc_auc: 0.7946 (LightGBM) → 24 Type II errors and 11 Type I errors → *35 total errors*\n",
    "3. Adaboost → roc_auc: 0.7888 (Adaboost)  → 25 Type II errors and 12 Type I errors → *37 total errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22659b0-e7fd-454a-939f-a1e9f297fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these results in a variable\n",
    "base_model_errors = {\"GBM\": {\"Type II(fn)\": 23, \"Type I(fp)\": 11, \"total\": 34},\n",
    "                    \"LightGBM\": {\"Type II(fn)\": 24, \"Type I(fp)\": 11, \"total\": 35},\n",
    "                    \"Adaboost\": {\"Type II(fn)\": 25, \"Type I(fp)\": 12,\"total\": 37}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994bcff-62d9-41b6-ae78-38d3ab6f87a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.2. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa80ef-f59a-44e5-9450-bbacfcab5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE HYPERPARAMETERS FOR OPTIMIZATION\n",
    "gbm_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [-1, 3, 5],\n",
    "    'num_leaves': [31, 40, 50],\n",
    "    'min_child_samples': [20, 30, 40],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "    \n",
    "adaboost_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850457b3-7bbd-4f7e-b6f9-25f9c8111611",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_tunned= [('GMB', GradientBoostingClassifier(random_state=random_state), gbm_params),\n",
    "                     (\"LightGBM\", LGBMClassifier(objective='binary', verbose=-1, random_state=random_state), lgbm_params),\n",
    "                     ('Adaboost', AdaBoostClassifier(random_state=random_state), adaboost_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9831591-93c1-40e3-b143-6ad0b714b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_optimize_models(X_train, y_train, X_test, y_test, classifiers_tunned, scoring=\"roc_auc\"):\n",
    "    \"\"\"\n",
    "    Evaluates and optimizes a set of classification models using cross-validation and grid search.\n",
    "\n",
    "    Parameters:\n",
    "    X_train : sparse matrix, shape (n_samples, n_features)\n",
    "        The training features.\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "        The training target values.\n",
    "    X_test : sparse matrix, shape (n_samples, n_features)\n",
    "        The test features.\n",
    "    y_test : array-like, shape (n_samples,)\n",
    "        The test target values.\n",
    "    classifiers_tunned : list of tuples\n",
    "        A list of tuples where each tuple contains a model name, model instance, and parameter grid.\n",
    "    scoring : string, default: \"roc_auc\"\n",
    "        The scoring metric for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function prints the mean cross-validation score before and after optimization, \n",
    "    the best parameters for each classifier, and displays the confusion matrix for the optimized models.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, classifier, param_grid in classifiers_tunned:\n",
    "        # Before optimization\n",
    "        initial_roc_auc = cross_val_score(classifier, X_train, y_train, cv=3, scoring=scoring).mean()\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring=scoring, cv=3)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # After optimization\n",
    "        optimized_roc_auc = cross_val_score(best_model, X_train, y_train, cv=3, scoring=scoring).mean()\n",
    "\n",
    "        # Predict and evaluate on the test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Print confusion matrix results\n",
    "        tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "        print(f\"True positives: {tp}, False positives: {fp}, False negatives: {fn}, True negatives: {tn}\")\n",
    "\n",
    "        # Print results\n",
    "        print(f\"roc_auc (Before): {initial_roc_auc:.4f}\")\n",
    "        print(f\"roc_auc (After): {optimized_roc_auc:.4f}\")\n",
    "        print(f\"{name} best params: {best_params}\")\n",
    "\n",
    "        # Plot the confusion matrix for the optimized model\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "        plt.title(f'Confusion matrix for {name} (Optimized)', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4645cc5-7c31-4fd3-af22-e61b8b0e036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_optimize_models(X_train, y_train, X_test, y_test, classifiers_tunned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dd7af-4c05-47d4-aec0-af6619beecb3",
   "metadata": {},
   "source": [
    "This **results are not satisfying** for my purpose. *For example, GBM* at first glance ROC-AUC has increased: roc_auc (Before): 0.8030 → roc_auc (After): 0.8239. However, my ambition is to get a model much more fit for purpose in the context of medicine, not for roc_auc metrics per se. Let us see in more details this result:\n",
    "\n",
    "\n",
    "1. **True positive**:  before 27 → after 24.\n",
    "2. **False positive**: before 13 → after 9.\n",
    "3. **False negative**: before 26 → after 29.\n",
    "4. **True negative**:  before 85 → after 89.\n",
    "\n",
    "As I said before, the worst case scenario is the point **3. FALSE NEGATIVE**, known as **Type II error**. My intention is to reduce this metric to a minimum, without spoiling the model to absurdity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83aeb8-300d-4069-8229-b07748aceab0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.3. Hyperparameter Optimization → Comprehension of the work field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72babe30-17ab-4c62-a39e-8aaf98677155",
   "metadata": {},
   "source": [
    "Here I outline the steps I will take to achieve my goal:\n",
    "1. Define a custom scoring function that penalizes false negatives more heavily.  → custom_scorer()\n",
    "2. Use this custom scoring function in the GridSearchCV.\n",
    "3. Evaluate and display the confusion matrix for the optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7157edd-2e85-406d-9423-ad5a86633f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scoring function to penalize false negatives more heavily\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp - 2 * fn  # Penalizing false negatives more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16ddd9-e7aa-48a3-8bc1-35c1d43e9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to use with GridSearchCV\n",
    "def custom_scorer_wrapper(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return custom_scorer(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee06a26-888c-4ee7-8daf-38f48c1c0575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a Custom Scorer for Cross-Validation:\n",
    "custom_scorer_cv = make_scorer(custom_scorer_wrapper, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bcc6ff-0696-4315-97f8-7130d444723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_threshold(probs, threshold):\n",
    "    return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd266b1a-5fe5-4c7a-916b-fedd171ad623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_optimize_models(X_train, y_train, X_test, y_test, classifiers_optimized, scoring=\"roc_auc\"):\n",
    "    for name, classifier, param_grid in classifiers_optimized:\n",
    "        # Before optimization\n",
    "        initial_roc_auc = cross_val_score(classifier, X_train, y_train, cv=3, scoring=scoring).mean()\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring=scoring, cv=3)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # After optimization\n",
    "        optimized_roc_auc = cross_val_score(best_model, X_train, y_train, cv=3, scoring=scoring).mean()\n",
    "\n",
    "        # Predict probabilities and find the best threshold\n",
    "        y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "        thresholds = np.linspace(0.2, 0.6, 9)  # tested with → np.linspace(0.1, 0.9, 9) → then np.linspace(0.1, 0.7, 9)\n",
    "        best_threshold = 0.5\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            y_pred_adjusted = adjust_threshold(y_probs, threshold)\n",
    "            score = custom_scorer(y_test, y_pred_adjusted)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "\n",
    "        y_pred_adjusted = adjust_threshold(y_probs, best_threshold)\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred_adjusted)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"roc_auc (Before): {initial_roc_auc:.4f}\")\n",
    "        print(f\"roc_auc (After): {optimized_roc_auc:.4f}\")\n",
    "        print(f\"{name} best params: {best_params}\")\n",
    "        print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "        \n",
    "        # Print confusion matrix results\n",
    "        tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "        print(f\"True positives: {tp}, False positives: {fp}, False negatives: {fn}, True negatives: {tn}\")\n",
    "\n",
    "        # Plot the confusion matrix for the optimized model\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "        plt.title(f'Confusion matrix for {name} (Optimized)', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f89fd-5752-46db-8233-38f3823e5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_optimize_models(X_train, y_train, X_test, y_test, classifiers_tunned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e3e5c-791a-4fc4-a76c-aec67f40766e",
   "metadata": {},
   "source": [
    "Given the context of working with the Pima diabetes dataset, my goal is to minimize Type II errors (false negatives) as much as possible, even if it means accepting a higher number of Type I errors (false positives). This approach ensures that I identify as many actual diabetes cases as possible, which is crucial for timely intervention and treatment. \n",
    "\n",
    "For this reason the best model that I found is **GBM** with the next params:\n",
    "- 'learning_rate': 0.01, \n",
    "- 'max_depth': 3, \n",
    "- 'min_samples_leaf': 1, \n",
    "- 'min_samples_split': 2, \n",
    "- 'n_estimators': 300, \n",
    "- 'subsample': 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8355e2c-e159-4112-95d5-360657aad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_errors = {'GBM': {'Type II(fn)': 6, 'Type I(fp)': 31, 'total': 37},\n",
    " 'LightGBM': {'Type II(fn)': 4, 'Type I(fp)': 36, 'total': 40},\n",
    " 'Adaboost': \"unacceptable model\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7450867-f97a-4d4a-b95b-6d4d5c9f39ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.4- Key Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6daab1-9247-494b-a8f4-1f012f1dbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BASE MODEL ERRORS\")\n",
    "print(pd.DataFrame(base_model_errors))\n",
    "print(\"************************************************\")\n",
    "print(\"OPTIMIZED MODEL ERRORS\")\n",
    "print(pd.DataFrame(optimized_model_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5639cc-3a62-4695-9360-0f3a6e347847",
   "metadata": {},
   "source": [
    "**Key Metrics to Consider**\n",
    "\n",
    "1. **Sensitivity** (Recall or True Positive Rate): Measures the proportion of actual positives that are correctly identified.\n",
    "    - $Sensitivity = \\frac{True Positives}{True Positives + False Negatives}$\n",
    "    - High sensitivity is crucial in medical diagnosis to ensure that most patients with the disease are identified.\n",
    "\n",
    "\n",
    "2. **Specificity** (True Negative Rate): Measures the proportion of actual negatives that are correctly identified.\n",
    "    - $Specificity = \\frac{True Negatives}{True Negatives + False Positives}$\n",
    "    - High specificity is important to ensure that healthy patients are not wrongly diagnosed with the disease.\n",
    "\n",
    "\n",
    "3. **Precision** (Positive Predictive Value): Measures the proportion of positive results that are true positives.\n",
    "    - $Precision = \\frac{True Positives}{True Positives + False Positives}$\n",
    "\n",
    "\n",
    "4. **Negative Predictive Value**: Measures the proportion of negative results that are true negatives.\n",
    "    - $Negative Predictive Value = \\frac{True Negatives}{True Negatives + False Negatives}$\n",
    "\n",
    "\n",
    "5. **F1 Score**: The harmonic mean of precision and recall, providing a single metric to balance them.\n",
    "    - $F1 Score = 2 x \\frac{Precision x Recall}{Precision + Recall}$\n",
    "\n",
    "\n",
    "​6. **ROC AUC**: The area under the ROC curve, which plots sensitivity against 1-specificity. Higher AUC values indicate better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff738d79-ceee-4c5e-8ce6-3583fec05084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(y_true, y_pred, y_probs):\n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    npv = tn / (tn + fn)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"True Positives: {tp}, False Positives: {fp}, False Negatives: {fn}, True Negatives: {tn}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Negative Predictive Value: {npv:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4daf22-e9fb-4a61-b988-697d5ba9520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_gbm = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "best_model = GradientBoostingClassifier(**best_params_gbm)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "threshold = 0.2  # Adjust as needed based on your earlier findings\n",
    "y_pred = (y_probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c1388-71e2-4f50-8e46-211d93d28c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model_performance(y_test, y_pred, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189a2ba-e08e-45dd-9f28-0938c83e960d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.5. Conclusion\n",
    "**1. High Sensitivity**: The model has a high sensitivity, which is good for identifying as many diabetic patients as possible. This is crucial in medical settings to ensure that most cases are caught early.\n",
    "\n",
    "**2. Moderate Specificity**: The specificity is moderate, meaning there are a **significant number of false positives**. While this can lead to unnecessary follow-up tests, it is often a trade-off to ensure high sensitivity. This has the **disadvantage** of **increasing** testing **costs**, but let's not forget that this is a study project where the main objective is to demonstrate my knowledge and skills as a data scientist and we can learn more from each other.\n",
    "\n",
    "**3. Balancing Act**: The goal in medical diagnostics is often to balance sensitivity and specificity based on the context and consequences of errors. In this case, the higher sensitivity (at the cost of specificity) might be acceptable to ensure fewer diabetic cases are missed.\n",
    "\n",
    "My current model with a threshold of 0.2 might be acceptable depending on the context and acceptable trade-offs for false negatives and false positives.\n",
    "However, let's not forget that they would have to be adjusted if necessary to adopt it to medical standards and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
