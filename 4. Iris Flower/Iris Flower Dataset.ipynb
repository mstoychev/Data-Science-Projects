{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7c048-3617-4027-a9cb-1a9018a7e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preproceesing\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f687177-db2f-46a3-bcc9-ab698103faee",
   "metadata": {},
   "source": [
    "# Iris Flower Dataset\n",
    "https://www.kaggle.com/datasets/arshid/iris-flower-dataset\n",
    "\n",
    "**Mitko Stoychev**\n",
    "\n",
    "10/06/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49876fc9-a1b0-4985-81ba-60f17859d74c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3e477-9e4c-446c-b763-a7c30dfe77fe",
   "metadata": {},
   "source": [
    "The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "This dataset became a typical test case for many statistical classification techniques in machine learning such as support vector machines\n",
    "\n",
    "**Features**:\n",
    "\n",
    "The dataset contains a set of 150 records under 5 attributes:\n",
    "1. SepalLengthCm\n",
    "2. SepalWidthCm\n",
    "3. PetalLengthCm\n",
    "4. PetalWidthCm\n",
    "5. Species\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YYiQed4kj_EZ2qfg_imDWA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dddc7b-ad61-4fad-890c-11d8a8e5fdd3",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb8b71-7d6e-4df2-9230-68f4483f6f2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04767de-85fb-4dc5-8672-0da047ec1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/iris.data.csv\", header=None)\n",
    "cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "data.columns = cols\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0b48c-8c94-4d3d-a46a-3fb1726c427b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2. Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3881a03-d875-4d3d-8291-1e8ceb8df629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isna().sum())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040d625-429f-4e4d-88b0-7f5e13b0b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43771124-1e56-4765-aa95-afecec861f8a",
   "metadata": {},
   "source": [
    "At first glance I can see that the dataset is very small (150 entries) and there are not any null or zero values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe5249-b5e0-4402-a7c5-95ceec0ecd13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3. Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a7750-8c0d-4a10-954a-5977cc2a744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467d10a-6463-4aae-9505-767b94734475",
   "metadata": {},
   "source": [
    "Data is free from duplicates values. Measuring 50 flowers and repeating 1 exact measurement with such small standard deviations is likely to happen.\n",
    "\n",
    "Conclusion: I don't have to tidy the data, nor clean it. The dataset is tidy and clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d7192-e5ec-4814-ae55-82b28946efcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4. Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b5b99-72a9-46c0-b09a-c44dbc999c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = data.select_dtypes(include=['object'])\n",
    "print(f\"The dataset has {categorical_col.shape[1]} categorical columns.\")\n",
    "\n",
    "categorical_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e8504-e46e-4e74-b3a2-aaffdc42a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_col.value_counts())\n",
    "print(\"*********************************************************************\")\n",
    "\n",
    "class_counts = data['class'].value_counts()\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Iris Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2ff97-9c64-4735-87e6-2a95f4699086",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.5. Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1051c-e73f-4072-8ecf-6180a16fc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = data.select_dtypes(include=[\"int\", \"float\"])\n",
    "print(f\"The dataset has {numerical_col.shape[1]} numerical columns.\")\n",
    "\n",
    "numerical_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38056c31-ed66-47a6-9eae-cc5f6203f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decribe method\n",
    "print(numerical_col.describe())\n",
    "\n",
    "# Heatmap to check any correlations\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(numerical_col.corr(), annot=True, linewidth=1, cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fdabe-6089-4b2a-ac74-9d2716f37b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=numerical_col)\n",
    "plt.title('Boxplot of Numerical Features in the Iris Dataset')\n",
    "\n",
    "medians = numerical_col.median()\n",
    "for idx, median in enumerate(medians):\n",
    "    plt.text(idx, median, f'{median:.2f}', \n",
    "             horizontalalignment='center', \n",
    "             size='medium', \n",
    "             color='black', \n",
    "             weight='semibold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141432e-6ae7-43b4-8366-045bb7df8530",
   "metadata": {},
   "source": [
    "We can note that \"Sepal Width\" has some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1042e4e-b4bc-4985-b9da-5c03cd0e41b4",
   "metadata": {},
   "source": [
    "### 2.7. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27274725-e591-4d74-b0e1-3d5fa25153f2",
   "metadata": {},
   "source": [
    "Bivariate analysis is a statistical method used to explore the relationship between two variables simultaneously. \n",
    "\n",
    "It focuses on understanding how changes in one variable are associated with changes in another variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54dbc5-3c16-434f-b3f9-7d83336809a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sepal Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b4ea9-2951-47b4-b791-fcc7329850e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"class/species\"\n",
    "data.groupby([\"class\"])[\"sepal_length\"].agg([\"mean\", \"median\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38188099-b868-458c-90b0-207f621ddc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x=\"class\", y=\"sepal_length\", color=\"lightblue\")\n",
    "\n",
    "# Calculate the median, Q1, and Q3 for each class\n",
    "grouped = data.groupby(\"class\")[\"sepal_length\"]\n",
    "medians = grouped.median()\n",
    "q1 = grouped.quantile(0.25)\n",
    "q3 = grouped.quantile(0.75)\n",
    "\n",
    "# Annotate the median, Q1, and Q3 values on the plot\n",
    "for i, class_name in enumerate(medians.index):\n",
    "    plt.text(i, medians[class_name], f\"{medians[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"medium\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q1[class_name], f'{q1[class_name]:.2f}', \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q3[class_name], f\"{q3[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "\n",
    "plt.title(\"Boxplot of Sepal Length by Iris Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ccee6-6fca-417e-86f7-d1776e4ab700",
   "metadata": {},
   "source": [
    "From the above visualization, Iris-virginica has the highest average sepal length (mean: 6.588), compared to Iris-versicolor (mean: 5.936) and Iris-setosa (mean: 5.006).\n",
    "\n",
    "The standard deviation is highest for Iris-virginica (0.635880), which indicates more variability in sepal length for this species. This could be related to the larger size of the plants, leading to more variation. It seems to be a totally natural behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35c256-3ea5-4f4b-a820-de270b4fa9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sepal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188150e-5407-44f0-ae08-624c02106b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"class/species\"\n",
    "data.groupby([\"class\"])[\"sepal_width\"].agg([\"mean\", \"median\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342d237-62c5-4f28-84ff-fbe447335c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x=\"class\", y=\"sepal_width\", color=\"deepskyblue\")\n",
    "\n",
    "# Calculate the median, Q1, and Q3 for each class\n",
    "grouped = data.groupby(\"class\")[\"sepal_width\"]\n",
    "medians = grouped.median()\n",
    "q1 = grouped.quantile(0.25)\n",
    "q3 = grouped.quantile(0.75)\n",
    "\n",
    "# Annotate the median, Q1, and Q3 values on the plot\n",
    "for i, class_name in enumerate(medians.index):\n",
    "    plt.text(i, medians[class_name], f\"{medians[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"medium\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q1[class_name], f'{q1[class_name]:.2f}', \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q3[class_name], f\"{q3[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "\n",
    "plt.title(\"Boxplot of Sepal Width by Iris Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b6450-cb87-456c-8598-29baf160c22c",
   "metadata": {},
   "source": [
    "From the above visualization, Iris-setosa has the highest average sepal width (mean: 3.418) compared to Iris-virginica (mean: 2.974) and Iris-versicolor (mean: 2.770). The species with the highest sepal width (Iris-setosa) is the one with the lowest sepal length. This might suggest different evolutionary adaptations for iris plants.\n",
    "\n",
    "We also note that there is no clear correlation between sepal length and sepal width based on the mean values across the species. This suggests that sepal length and width may vary independently of each other.\n",
    "\n",
    "As a last note, Iris Virginica is the only species that presents outliers among the measurements of the sepals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8515a-7299-4ce5-8c9b-520a794b74ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Petal Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f6610-8d24-477a-b85f-28567732e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"class/species\"\n",
    "data.groupby([\"class\"])[\"petal_length\"].agg([\"mean\", \"median\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1da1a-39a3-4a9a-b1a1-855ce9c349ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=data, x=\"class\", y=\"petal_length\", color=\"seagreen\")\n",
    "\n",
    "# Calculate the median, Q1, and Q3 for each class\n",
    "grouped = data.groupby(\"class\")[\"petal_length\"]\n",
    "medians = grouped.median()\n",
    "q1 = grouped.quantile(0.25)\n",
    "q3 = grouped.quantile(0.75)\n",
    "\n",
    "# Annotate the median, Q1, and Q3 values on the plot\n",
    "for i, class_name in enumerate(medians.index):\n",
    "    plt.text(i, medians[class_name], f\"{medians[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"medium\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q1[class_name], f'{q1[class_name]:.2f}', \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q3[class_name], f\"{q3[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "\n",
    "plt.title(\"Boxplot of Petal Length by Iris Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07488d7-6f95-4bf2-a918-2c490b34db7c",
   "metadata": {},
   "source": [
    "Iris-virginica has the highest average petal length (mean: 5.552) compared to Iris-versicolor (mean: 4.260) and Iris-setosa (mean: 1.464).\n",
    "\n",
    "The standard deviation is highest for Iris-virginica (0.551895), indicating more variability in petal length for this species, likely related to the larger size of the plants. Natural behaviour similar to sepal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15852ca5-7deb-41b8-a3bf-a31afab64620",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Petal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2aed0-c91a-4ea1-bd7d-62b4162a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING BY \"class/species\"\n",
    "data.groupby([\"class\"])[\"petal_width\"].agg([\"mean\", \"median\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf97aa-a2a1-4c87-ae73-6e030f13d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=data, x=\"class\", y=\"petal_width\", color=\"green\")\n",
    "\n",
    "# Calculate the median, Q1, and Q3 for each class\n",
    "grouped = data.groupby(\"class\")[\"petal_width\"]\n",
    "medians = grouped.median()\n",
    "q1 = grouped.quantile(0.25)\n",
    "q3 = grouped.quantile(0.75)\n",
    "\n",
    "# Annotate the median, Q1, and Q3 values on the plot\n",
    "for i, class_name in enumerate(medians.index):\n",
    "    plt.text(i, medians[class_name], f\"{medians[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"medium\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q1[class_name], f'{q1[class_name]:.2f}', \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "    \n",
    "    plt.text(i, q3[class_name], f\"{q3[class_name]:.2f}\", \n",
    "             horizontalalignment=\"center\", \n",
    "             size=\"small\", \n",
    "             color=\"black\", \n",
    "             weight=\"semibold\")\n",
    "\n",
    "plt.title(\"Boxplot of Petal Width by Iris Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253a1df-7efa-4665-a4cc-bd3b7f32e4bd",
   "metadata": {},
   "source": [
    "Iris-virginica has the highest average petal width (mean: 2.026) compared to Iris-versicolor (mean: 1.326) and Iris-setosa (mean: 0.244).\n",
    "\n",
    "The species with the highest petal width (Iris-virginica) also has the highest petal length. This might suggest a correlation between larger petal dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c943daf-9c78-4978-b90d-85d7060f6b83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Histogram distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9496855-7aa1-40b5-842d-bb93c2072693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot histogram for Sepal Length\n",
    "sns.histplot(data[\"sepal_length\"], ax=axes[0, 0], color=\"b\")\n",
    "axes[0, 0].set_title(\"Sepal Length\")\n",
    "\n",
    "# Plot histogram for Sepal Width\n",
    "sns.histplot(data[\"sepal_width\"], ax=axes[0, 1], color=\"g\")\n",
    "axes[0, 1].set_title(\"Sepal Width\")\n",
    "\n",
    "# Plot histogram for Petal Length\n",
    "sns.histplot(data[\"petal_length\"], ax=axes[1, 0], color=\"r\")\n",
    "axes[1, 0].set_title(\"Petal Length\")\n",
    "\n",
    "# Plot histogram for Petal Width\n",
    "sns.histplot(data[\"petal_width\"], ax=axes[1, 1], color=\"m\")\n",
    "axes[1, 1].set_title(\"Petal Width\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77186755-3928-43c5-ba89-3ff53b1b5248",
   "metadata": {},
   "source": [
    "Upon reviewing the plots above, it's evident that the distribution \"Petal Length (cm)\" and \"Petal Width (cm)\" does not follow a Gaussian distribution. \n",
    "\n",
    "Therefore, our next step is to transform the distribution of Numerical Features to adhere to a Gaussian distribution, before starting with the modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa1b29-144e-4a84-a842-0361ce222793",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.8. Normalization and Scaling the Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca0e5c-f768-4645-9da1-3b76340c9f91",
   "metadata": {},
   "source": [
    "**Data Normalization**: For models that benefit from normally distributed data (e.g., linear regression, LDA).\n",
    "\n",
    "**Scaling Data**: Regardless of the distribution, scaling (standardization or normalization) is beneficial for most models, especially those relying on distance metrics (e.g., KNN, SVM) or gradient-based optimization (e.g., neural networks).\n",
    "\n",
    "**Model Choice**: Choose the model based on the nature of data and the problem at hand. Models like Random Forests and GBMs are robust to various data distributions and often perform well without the need for strict data transformations.\n",
    "\n",
    "\n",
    "However I will test with 10 different algorithms and I decide to Normalize and then Scale the data. That´s the reason of my choise. My approach is the next:\n",
    "1. Due to \"Petal Length (cm)\" and \"Petal Width (cm)\" does not follow Gaussian Distribution I decide to apply **QuantileTransformation** to them.\n",
    "2. Then I will apply standard Scaler to to all the features.ta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3515b-10ca-4f02-b693-07c911d8bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and class\n",
    "features = data.drop(columns=\"class\")\n",
    "class_labels = data[\"class\"]\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=0,  n_quantiles=150) #QuantileTransformer uses 1000 quantiles by default\n",
    "features[[\"petal_length\", \"petal_width\"]] = quantile_transformer.fit_transform(features[[\"petal_length\", \"petal_width\"]])\n",
    "\n",
    "# Apply Standard Scaling to all features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "scaled_features_df[\"class\"] = class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d7873-5a2b-4e0d-aa0d-fdb7993f7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histograms after normalization and scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "sns.histplot(scaled_features_df[\"sepal_length\"], ax=axes[0, 0], color=\"b\")\n",
    "axes[0, 0].set_title(\"Standard Scaled Sepal Length\")\n",
    "\n",
    "sns.histplot(scaled_features_df[\"sepal_width\"], ax=axes[0, 1], color=\"g\")\n",
    "axes[0, 1].set_title(\"Standard Scaled Sepal Width\")\n",
    "\n",
    "sns.histplot(scaled_features_df[\"petal_length\"], ax=axes[1, 0], color=\"r\")\n",
    "axes[1, 0].set_title(\"Quantile Transformed & Standard Scaled Petal Length\")\n",
    "\n",
    "sns.histplot(scaled_features_df[\"petal_width\"], ax=axes[1, 1], color=\"m\")\n",
    "axes[1, 1].set_title(\"Quantile Transformed & Standard Scaled Petal Width\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b6556-7263-4e34-807e-f8386ca14461",
   "metadata": {},
   "source": [
    "These distributions look much more like a Normal Distribution. It looks like we can start with the automated learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d3ea-39c3-4a93-a7fb-3c80fdd10551",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f0a96-9436-4c43-8a77-a69046ed7d54",
   "metadata": {},
   "source": [
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a **test set**:  *X_test, y_test*.\n",
    "\n",
    "**Train Test Split**: Split the data into a training and test sets. We hide the test set from the algorith to have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n",
    "\n",
    "![Train Test Split](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_train-test-split_0.jpg)\n",
    "\n",
    "The distribution of the target variable in the original dataset is balanced, that is, every species represent 33,3% of the data. But randomness is randomness and I want to ensure that the train and test sets have a similar distribution, then using stratified sampling would be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be9c3c-f168-4f32-a84d-9c27dbe45a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfed499-1e3f-45b9-a1ee-52c7216336af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_features_df.drop(columns=[\"class\"])    #Features\n",
    "y = scaled_features_df[\"class\"]    # target variable\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61272de3-c880-412c-9c65-2c0f0fa48dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original class labels and their corresponding numerical values\n",
    "print(\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41398dae-2b32-48b9-82b1-08833b0711fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d399ed1-0b54-4605-b858-1a99134528b0",
   "metadata": {},
   "source": [
    "## 4. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07743811-111c-42cc-aada-66a8bef11f45",
   "metadata": {},
   "source": [
    "Model building is the process of creating and refining mathematical representations, typically using algorithms or statistical techniques, to make predictions, classifications, or gain insights from data. It involves selecting appropriate features, choosing a suitable algorithm, training the model on labeled data, and evaluating its performance using metrics like accuracy or mean squared error. Model building requires iterative experimentation, parameter tuning, and validation to ensure the resulting model generalizes well to unseen data. It is a fundamental step in machine learning and data analysis, enabling informed decision-making, pattern recognition, and predictive analytics across various domains and industries.\n",
    "\n",
    "In this notebook I am going to use **10 of the most popular classifiers**:\n",
    "- Logistic Regression (LR)\n",
    "- K-Neighbors Classifier (KNN) \n",
    "- Support Vector Classifier (SVC)\n",
    "- Decision Tree Classifier (DTC)\n",
    "- Random Forest Classifier (RFC)\n",
    "- Ada Boost Classifier (Adaboost)\n",
    "- Gradient Boosting Classifier (‎GBM)\n",
    "- XGB Classifier (XGBoost)\n",
    "- LightGBM Classifier (LightGBM)\n",
    "- Cat Boost Classifier (CatBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db62e5b-5c72-413b-9a61-6f8b92e6f9b9",
   "metadata": {},
   "source": [
    "### 4.1. Base Model Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be294c6c-bae6-4bf9-9161-3280ab993668",
   "metadata": {},
   "source": [
    "**Cross Validation** is a widely used technique for evaluating model performance and subsequent trend checking in machine learning. It consists of partitioning the data into subsets (parts), where one subset is used for training and another subset is used to test and evaluate model performance.\n",
    "- The partitioned data is the training data (80% of the original data)\n",
    "- K-fold: the training set is randomly divided into K subsets of approximately the same size, then K iterations are performed, where K-1 subsets are for training and 1 for validation.\n",
    "- The performance computed by k-fold cross-validation is the average of all the values computed in each iteration.\n",
    "\n",
    "\n",
    "**Grid Search Cross Validation**\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"Grid Search Cross Validation\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5c93e-d570-4d86-af15-a80b480c7c54",
   "metadata": {},
   "source": [
    "**The confusion matrix**\n",
    "\n",
    "The confusion matrix is a technique used for summarizing the performance of a classification algorithm (binary outputs).\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*9r99oJ2PTRi4gYF_.jpg\" alt=\"The confusion Matrix\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35115859-35e9-4853-beda-1d02da76e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334328c-abf2-47d0-8c08-f6dd4a4b6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [(\"LR\", LogisticRegression(random_state=random_state)),\n",
    "              (\"KNN\", KNeighborsClassifier()),\n",
    "              (\"SVC\", SVC(random_state=random_state)),\n",
    "              (\"DTC\", DecisionTreeClassifier(random_state=random_state)),\n",
    "              (\"RFC\", RandomForestClassifier(random_state=random_state)),\n",
    "              (\"Adaboost\", AdaBoostClassifier(random_state=random_state)),\n",
    "              (\"GBM\", GradientBoostingClassifier(random_state=random_state)),\n",
    "              (\"LightGBM\", LGBMClassifier(objective=\"multiclass\", verbose=-1, random_state=random_state)),  # NOT BINARY\n",
    "              (\"GXBoost\", XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_state)), # mlogloss for multiclass problems\n",
    "              (\"CatBoost\", CatBoostClassifier(verbose=False, random_state=random_state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d51fb4-9b20-42a6-bceb-913f082b4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_models(X_train, X_test, y_train, y_test, classifiers, scoring=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Evaluates a set of classification models using cross-validation and plots confusion matrices.\n",
    "\n",
    "    Parameters:\n",
    "    X_train : sparse matrix, shape (n_samples, n_features)\n",
    "        The training features.\n",
    "    X_test : sparse matrix, shape (n_samples, n_features)\n",
    "        The testing features.\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "        The training target values.\n",
    "    y_test : array-like, shape (n_samples,)\n",
    "        The testing target values.\n",
    "    classifiers: list of tuples\n",
    "        List of (name, model) tuples of the classifiers to evaluate.\n",
    "    scoring : string, default: \"roc_auc\"\n",
    "        Scoring metric to evaluate the models.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function prints the mean cross-validation score for each classifier and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    for name, classifier in classifiers:\n",
    "        # Cross-validate the model\n",
    "        cv_results = cross_validate(classifier, X_train, y_train, cv=3, scoring=scoring)\n",
    "        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name})\")\n",
    "\n",
    "        # Train the model on the entire training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1-score:\", f1)\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Print confusion matrix results for multi-class classification\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cnf_matrix)\n",
    "        \n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt=\"g\")\n",
    "        plt.title(f\"Confusion matrix for {name}\", y=1.1)\n",
    "        plt.ylabel(\"Actual label\")\n",
    "        plt.xlabel(\"Predicted label'\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f0bab-a191-4642-a0d6-4b013c05aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the base models\n",
    "base_models(X_train, X_test, y_train, y_test, classifiers, \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac135413-7e56-4af7-9547-f5faace6d1ad",
   "metadata": {},
   "source": [
    "I have done this experiment with cv=2, cv=3, cv=5, cv=10 ...\n",
    "\n",
    "Given the size of the dataset and the consistency of performance across different cross-validation folds, Logistic Regression (LR) seems like a reasonable choice. \n",
    "\n",
    "LR is generally computationally efficient, interpretable, and suitable for small datasets. It also tends to perform well when the relationship between features and the target variable is relatively linear, which could be the case with the Iris dataset.\n",
    "\n",
    "Since LR has already demonstrated strong performance and simplicity, I could start by tuning its hyperparameters using techniques like grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5d1d0-66ec-4dd7-8f36-0ac5167b57dc",
   "metadata": {},
   "source": [
    "## 4.2. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dcadc-4d85-482c-bcf1-ad9ad35254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ce664-d3fd-4bb7-a8db-3be52bb21774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Logistic Regression model\n",
    "lr_base = LogisticRegression(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3f948-f6c5-464a-8c26-55fdbbd5b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuned_model(classifier, param_grid, X_train, y_train, y_test, cv=3, scoring='accuracy'):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=cv, scoring=scoring)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best hyperparameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    \n",
    "    # Evaluate model using cross-validation\n",
    "    scores = cross_validate(best_model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "    print(\"***************************************************************************************************************************\")\n",
    "    print(\"Cross-Validation Scores:\", scores)\n",
    "    print(\"***************************************************************************************************************************\")\n",
    "    print(\"Mean Cross-Validation Scores:\", round(scores[\"test_score\"].mean(), 4))\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "        \n",
    "    # Compute the confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print confusion matrix results for multi-class classification\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cnf_matrix)\n",
    "        \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt=\"g\")\n",
    "    plt.title(f\"Confusion matrix for Tunned Linear Regression\", y=1.1)\n",
    "    plt.ylabel(\"Actual label\")\n",
    "    plt.xlabel(\"Predicted label'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a6b41-ee46-4e00-acfc-5395148a93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tuned_model function\n",
    "tuned_lr_model = tuned_model(lr_base, param_grid, X_train, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638b22a-f8f5-4583-bce5-02429657bfbf",
   "metadata": {},
   "source": [
    "It seems like the model is performing quite well with an accuracy of approximately 96.67%. \n",
    "\n",
    "The confusion matrix indicates that most of the predictions are correct, with only one misclassification in the second row and third column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
